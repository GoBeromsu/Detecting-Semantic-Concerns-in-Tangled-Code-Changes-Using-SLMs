experiment_name: "RQ2_Accuracy_vs_Context_Length"
dataset_name: "Berom0227/Untangling-Multi-Concern-Commits-with-Small-Language-Models"
dataset_split: "test"
models:
  slm:
    - "microsoft/phi-4"
  llm:
    - "gpt-4-turbo"
include_message: true
context_lengths: [256, 512, 1024, 2048, 4096]
output_dir: "../results/rq2"
temperature: 0
max_tokens: 16384
